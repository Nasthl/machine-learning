{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":24196,"sourceType":"datasetVersion","datasetId":4427},{"sourceId":3559777,"sourceType":"datasetVersion","datasetId":2138486}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Bloki residualne model \n","metadata":{}},{"cell_type":"markdown","source":"### Zadanie Implementacyjne: Budowa Modelu Klasyfikacyjnego z Wykorzystaniem Bloków Residualnych na cifar 100\nCel Zadania:\nCelem zadania jest zaimplementowanie modelu klasyfikacyjnego, wykorzystującego bloki residualne, które są popularną architekturą w sieciach konwolucyjnych. Model zostanie zbudowany na podstawie zapewnionego kodu, który zawiera definicje bloku residualnego oraz funkcję budującą model.\n\n##### Opis Zadania:\nAnaliza Kodu: Przeanalizuj dostarczony kod, który zawiera implementację bloku residualnego oraz funkcji budującej model klasyfikacyjny.\n\n* Zwróć uwagę na strukturę bloku residualnego. Jakie operacje są wykonywane w tym bloku?\n* Przeanalizuj funkcję build_residual_model. Jakie warstwy są używane w modelu klasyfikacyjnym? Jakie są parametry tych warstw?\n##### Implementacja Modelu:\n\n* Wykorzystaj dostarczony kod do zaimplementowania modelu klasyfikacyjnego z wykorzystaniem bloków residualnych.\n* W przypadku potrzeby dostosuj parametry modelu, takie jak rozmiar wejściowy czy liczba klas wyjściowych, aby pasowały do Twojego zadania klasyfikacji.\n* Przygotowanie Danych:\n\n* Wybierz lub przygotuj zbiór danych do treningu i ewaluacji modelu. Może to być zbiór danych zawierający obrazy należące do różnych klas.\n* Przygotuj dane: przeskaluj obrazy do odpowiedniego rozmiaru i przekonwertuj etykiety klas na odpowiedni format (np. one-hot encoding).\n##### Trening Modelu:\n\n* Skompiluj model, wybierając odpowiedni optymalizator i funkcję straty. Możesz wykorzystać optymalizator 'adam' i funkcję straty 'categorical_crossentropy', jeśli zadanie klasyfikacji ma więcej niż dwie klasy.\n* Przeprowadź trening modelu na przygotowanym zbiorze danych. Ustaw odpowiednią liczbę epok treningu.\n##### Ewaluacja Modelu:\n\nPo treningu ocen skuteczność modelu na zestawie testowym. Użyj metryk, takich jak dokładność (accuracy), aby ocenić wydajność modelu.\nZwizualizuj wyniki ewaluacji. ","metadata":{}},{"cell_type":"code","source":"from keras.datasets import cifar100\nfrom keras.utils import to_categorical\n\n(X_train, y_train), (X_test, y_test) = cifar100.load_data()\n\ny_train = to_categorical(y_train, num_classes=100)\ny_test = to_categorical(y_test, num_classes=100)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T17:04:22.940144Z","iopub.execute_input":"2024-03-05T17:04:22.940554Z","iopub.status.idle":"2024-03-05T17:04:41.145261Z","shell.execute_reply.started":"2024-03-05T17:04:22.940523Z","shell.execute_reply":"2024-03-05T17:04:41.144447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.datasets import cifar100\n# example of loading the cifar100 dataset\nfrom matplotlib import pyplot\n\n# load dataset\n# summarize loaded dataset\nprint('Train: X=%s, y=%s' % (X_train.shape, y_train.shape))\nprint('Test: X=%s, y=%s' % (X_test.shape, y_test.shape))\n# plot first few images\nfor i in range(9):\n\t# define subplot\n\tpyplot.subplot(330 + 1 + i)\n\t# plot raw pixel data\n\tpyplot.imshow(X_train[i])\n# show the figure\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-05T17:04:53.611875Z","iopub.execute_input":"2024-03-05T17:04:53.612829Z","iopub.status.idle":"2024-03-05T17:04:54.542802Z","shell.execute_reply.started":"2024-03-05T17:04:53.612795Z","shell.execute_reply":"2024-03-05T17:04:54.541899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalizuj dane wejściowe\nX_train = X_train.astype('float32') / 255\nX_test = X_test.astype('float32') / 255","metadata":{"execution":{"iopub.status.busy":"2024-03-05T17:04:58.277823Z","iopub.execute_input":"2024-03-05T17:04:58.278449Z","iopub.status.idle":"2024-03-05T17:04:58.536049Z","shell.execute_reply.started":"2024-03-05T17:04:58.278404Z","shell.execute_reply":"2024-03-05T17:04:58.535243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# todo model \n\ndef residual_block(input_tensor, filters, kernel_size):\n   \"\"\"\n   Tworzy prosty blok residualny.\n\n   Parametry:\n   input_tensor - tensor wejściowy do bloku.\n   filters - liczba filtrów w warstwach konwolucyjnych.\n   kernel_size - rozmiar jądra w warstwach konwolucyjnych.\n  \n   Zwraca:\n   Tensor wyjściowy z bloku residualnego.\n   \"\"\"\n   x = layers.Conv2D(filters, kernel_size, padding='same')(input_tensor)\n   x = layers.BatchNormalization()(x)\n   x = layers.Activation('relu')(x)\n\n   x = layers.Conv2D(filters, kernel_size, padding='same')(x)\n   x = layers.BatchNormalization()(x)\n\n   x = layers.add([x, input_tensor])\n   x = layers.Activation('relu')(x)\n\n   return x\n\ndef build_residual_model(input_shape, num_classes):\n   inputs = layers.Input(shape=input_shape)\n   # Wstępna warstwa konwolucyjna\n   x = layers.Conv2D(64, 7, strides=2, padding='same')(inputs)\n   x = layers.BatchNormalization()(x)\n   x = layers.Activation('relu')(x)\n   x = layers.MaxPooling2D(pool_size=3, strides=2, padding='same')(x)\n   # Bloki residualne\n   x = residual_block(x, filters=64, kernel_size=3)\n   x = residual_block(x, filters=64, kernel_size=3)\n   x = residual_block(x, filters=64, kernel_size=3)\n   # Warstwa kończąca do klasyfikacji\n   x = layers.GlobalAveragePooling2D()(x)\n   outputs = layers.Dense(num_classes, activation='softmax')(x)\n\n   model = models.Model(inputs=inputs, outputs=outputs)\n   model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n   return model\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-05T17:05:15.199297Z","iopub.execute_input":"2024-03-05T17:05:15.199656Z","iopub.status.idle":"2024-03-05T17:05:15.211021Z","shell.execute_reply.started":"2024-03-05T17:05:15.199629Z","shell.execute_reply":"2024-03-05T17:05:15.209983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.shape","metadata":{"execution":{"iopub.status.busy":"2024-03-05T17:05:56.047109Z","iopub.execute_input":"2024-03-05T17:05:56.047489Z","iopub.status.idle":"2024-03-05T17:05:56.053707Z","shell.execute_reply.started":"2024-03-05T17:05:56.047457Z","shell.execute_reply":"2024-03-05T17:05:56.052758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\n\n# Przykładowe parametry\ninput_shape = (32, 32, 3)\nnum_classes = 100  # Zmień na odpowiednią liczbę klas dla swojego przypadku\n\n# Budowanie i podsumowanie modelu\nmodel = build_residual_model(input_shape, num_classes)\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-05T17:15:15.927235Z","iopub.execute_input":"2024-03-05T17:15:15.929055Z","iopub.status.idle":"2024-03-05T17:15:16.187849Z","shell.execute_reply.started":"2024-03-05T17:15:15.928974Z","shell.execute_reply":"2024-03-05T17:15:16.186927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmodel.fit(X_train,y_train,verbose=1,batch_size=32, validation_split=0.2, epochs=10)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T17:08:13.156997Z","iopub.execute_input":"2024-03-05T17:08:13.157367Z","iopub.status.idle":"2024-03-05T17:09:02.466585Z","shell.execute_reply.started":"2024-03-05T17:08:13.157339Z","shell.execute_reply":"2024-03-05T17:09:02.465803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train,y_train,verbose=1,batch_size=64, validation_split=0.2, epochs=50, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T17:15:19.954326Z","iopub.execute_input":"2024-03-05T17:15:19.954691Z","iopub.status.idle":"2024-03-05T17:18:05.338274Z","shell.execute_reply.started":"2024-03-05T17:15:19.954664Z","shell.execute_reply":"2024-03-05T17:18:05.337456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Few shot learning ","metadata":{}},{"cell_type":"markdown","source":"### Zadanie Implementacyjne: Klasyfikacja Obrazów z Wykorzystaniem Few-Shot Learning i ResNet50\n","metadata":{"execution":{"iopub.status.busy":"2024-03-04T20:08:33.181298Z","iopub.execute_input":"2024-03-04T20:08:33.18165Z","iopub.status.idle":"2024-03-04T20:08:33.185722Z","shell.execute_reply.started":"2024-03-04T20:08:33.181626Z","shell.execute_reply":"2024-03-04T20:08:33.184902Z"}}},{"cell_type":"markdown","source":"Celem zadania jest opracowanie modelu klasyfikacji obrazów z użyciem techniki few-shot learning przy wykorzystaniu wstępnie wytrenowanego modelu ResNet50. Zadanie skupia się na wyzwaniu klasyfikacji, gdy dostępna jest bardzo ograniczona liczba przykładów dla każdej z klasyfikowanych kategorii.\n\n","metadata":{}},{"cell_type":"markdown","source":"### Przygotowanie Danych:\n\n1. Wybierz zbiór danych do klasyfikacji. Może to być dowolny zbiór danych zawierający obrazy należące do różnych klas. Przykładowo, można wybrać zbiór danych zawierający zdjęcia zwierząt, pojazdów czy obiekty naturalne.\nPrzygotuj dane: przeskaluj obrazy do rozmiaru 224x224 pikseli, przekonwertuj etykiety klas na format one-hot encoding, podziel zbiór danych na zestawy treningowy, walidacyjny i testowy.\n### Modyfikacja i Implementacja Modelu:\n\n2. Wykorzystaj wstępnie wytrenowany model ResNet50, jak pokazano w dostarczonym kodzie. \n* Model ten powinien zostać załadowany bez górnych warstw, aby umożliwić dodanie nowych warstw klasyfikacyjnych.\n* Zablokuj trening wszystkich wcześniejszych warstw ResNet50, aby zapobiec ich modyfikacji podczas pierwszego etapu treningu.\n* Dodaj warstwy klasyfikacyjne na końcu modelu. \nW przykładzie zastosowano GlobalAveragePooling2D i warstwę Dense z aktywacją softmax, ale zachęcamy do eksperymentowania z różnymi konfiguracjami.\n### Trening Modelu:\n\n3. Skompiluj model, używając optymalizatora Adam. \n* Jako funkcję straty wybierz 'categorical_crossentropy', jeśli Twoje zadanie klasyfikacji ma więcej niż dwie klasy.\n* Przeprowadź trening modelu na przygotowanym zbiorze danych. Ustaw odpowiednią liczbę epok (w przykładzie jest tylko 1 dla demonstracji, jednak w praktycznych zastosowaniach warto użyć więcej).\n* Monitoruj wyniki treningu i walidacji, aby dostosować parametry treningu i uniknąć przetrenowania.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-04T20:09:31.218514Z","iopub.execute_input":"2024-03-04T20:09:31.218843Z","iopub.status.idle":"2024-03-04T20:09:31.228283Z","shell.execute_reply.started":"2024-03-04T20:09:31.218816Z","shell.execute_reply":"2024-03-04T20:09:31.226944Z"}}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import cifar100\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.layers import Input, Dense, GlobalAveragePooling2D, AveragePooling2D, Flatten\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n","metadata":{"execution":{"iopub.status.busy":"2024-03-05T17:20:33.126003Z","iopub.execute_input":"2024-03-05T17:20:33.126825Z","iopub.status.idle":"2024-03-05T17:20:33.147844Z","shell.execute_reply.started":"2024-03-05T17:20:33.126794Z","shell.execute_reply":"2024-03-05T17:20:33.146761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Definicja generatora obrazów\ndatagen = ImageDataGenerator(\n    rescale=1.0/255,           # Przeskalowanie wartości pikseli do zakresu [0, 1]\n\n)\n\n# Wczytanie danych treningowych\ntrain_generator = datagen.flow_from_directory(\n    '/kaggle/input/urban-and-rural-photos/train',\n    target_size=(224, 224),    # Rozmiar docelowy obrazów\n    batch_size=12,             # Rozmiar wsadu\n    class_mode='binary',  # Rodzaj klasyfikacji (binary lub categorical)\n    subset='training'          # Ustawienie dla danych treningowych\n)\n\n\n# Wczytanie danych treningowych\ntest_generator = datagen.flow_from_directory(\n    '/kaggle/input/urban-and-rural-photos/val',\n    target_size=(224, 224),    # Rozmiar docelowy obrazów\n    batch_size=20,             # Rozmiar wsadu\n    class_mode='binary',  # Rodzaj klasyfikacji (binary lub categorical)\n    subset='validation'          # Ustawienie dla danych treningowych\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T17:31:47.169193Z","iopub.execute_input":"2024-03-05T17:31:47.170049Z","iopub.status.idle":"2024-03-05T17:31:47.191622Z","shell.execute_reply.started":"2024-03-05T17:31:47.170020Z","shell.execute_reply":"2024-03-05T17:31:47.190853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Parametry\nimage_size = 224  # Zakładając, że używamy tego rozmiaru obrazu\nnum_classes = 2  # Dla przykładu, zakładamy że mamy 2 klasy: miejskie i wiejskie\n\n\n# Utworzenie generatora danych bez augmentacji dla zbioru walidacyjnego\nvalidation_data_generator = ImageDataGenerator(rescale=1./255)  # Tylko normalizacja pikseli\n\n# Konfiguracja generatora treningowego z augmentacją\ntrain_data_generator = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\ntrain_generator = train_data_generator.flow_from_directory(\n    '../input/urban-and-rural-photos/train',\n    target_size=(image_size, image_size),\n    batch_size=12,\n    class_mode='categorical')\n\n# Konfiguracja generatora walidacyjnego bez augmentacji\nvalidation_generator = validation_data_generator.flow_from_directory(\n    '../input/urban-and-rural-photos/val',\n    target_size=(image_size, image_size),\n    batch_size=20,\n    class_mode='categorical')","metadata":{"execution":{"iopub.status.busy":"2024-03-05T17:33:21.742645Z","iopub.execute_input":"2024-03-05T17:33:21.743453Z","iopub.status.idle":"2024-03-05T17:33:21.765362Z","shell.execute_reply.started":"2024-03-05T17:33:21.743399Z","shell.execute_reply":"2024-03-05T17:33:21.764532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Dense,Dropout\nfrom tensorflow.keras.optimizers import Adam\noutput_logistic_regression = 2\nfrom tensorflow.keras.applications import ResNet152\n\nbase_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\nfor layer in base_model.layers:\n    layer.trainable = False\n\n# Adding new classification layers\nmodel = Sequential([   base_model,   GlobalAveragePooling2D(),   Dense(output_logistic_regression, activation='softmax')])\nmodel.compile(optimizer=Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n\nmodel.fit(train_generator, epochs=19, verbose=1, validation_data=validation_generator)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T17:37:36.634366Z","iopub.execute_input":"2024-03-05T17:37:36.634761Z","iopub.status.idle":"2024-03-05T17:38:15.641800Z","shell.execute_reply.started":"2024-03-05T17:37:36.634729Z","shell.execute_reply":"2024-03-05T17:38:15.640876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(train_generator, epochs=10, verbose=1, validation_data=validation_generator)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(train_generator, epochs=3, verbose=1, validation_data=validation_generator)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Segmentation ","metadata":{}},{"cell_type":"markdown","source":"### Zadanie: Implementacyjne: U-Net dla Segmentacji Maski Płuc\n\nCelem zadania jest implementacja i trening modelu U-Net na zbiorze danych zawierającym obrazy RTG klatki piersiowej (chest X-ray) wraz z maskami płuc. Zadanie skupia się na praktycznym zastosowaniu modelu U-Net do segmentacji obrazów medycznych, co jest kluczowym zadaniem w analizie medycznej obrazów diagnostycznych.\n\n","metadata":{}},{"cell_type":"markdown","source":"Celem zadania jest implementacja i trening modelu U-Net na zbiorze danych zawierającym obrazy RTG klatki piersiowej (chest X-ray) wraz z maskami płuc. Zadanie skupia się na praktycznym zastosowaniu modelu U-Net do segmentacji obrazów medycznych, co jest kluczowym zadaniem w analizie medycznej obrazów diagnostycznych.\n\nOpis Zadania:\n1. Przygotowanie Danych:\n    * Wczytaj dostępny zbiór danych zawierający obrazy RTG klatki piersiowej wraz z odpowiadającymi im maskami płuc. \n    * Przygotuj dane do treningu: przeskaluj obrazy i maski do rozmiaru 128x128 pikseli (lub innego, jeśli uznasz to za stosowne), przekonwertuj maski do binarnej formy, gdzie piksele należące do płuc mają wartość 1, a tło 0.\n    * Podziel zbiór danych na zestawy: treningowy, walidacyjny i testowy.\n2. Implementacja Modelu U-Net:\n\n    * Wykorzystaj dostarczony kod modelu U-Net. Dokonaj ewentualnych modyfikacji, jeśli jest to konieczne, aby dostosować model do Twoich potrzeb.\n    * Zaimplementuj model w środowisku programistycznym wspierającym TensorFlow/Keras.\n3. Trening Modelu:\n    * Skompiluj model, używając optymalizatora 'adam' i funkcji straty 'binary_crossentropy'. Jako metrykę wybierz 'accuracy'.\n4. Ewaluacja Modelu:\n    * Oceń skuteczność modelu na zbiorze testowym, wykorzystując odpowiednie metryki, takie jak dokładność.\n    * Zwizualizuj wyniki segmentacji na wybranych obrazach z zestawu testowego, porównując maski przewidywane przez model z maskami rzeczywistymi.\n","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Ścieżki do folderów z danymi\ntrain_image_dir = '/kaggle/input/lung-mask-image-dataset/ChestXray/train/image'\ntrain_mask_dir = '/kaggle/input/lung-mask-image-dataset/ChestXray/train/mask'\ntest_image_dir = '/kaggle/input/lung-mask-image-dataset/ChestXray/test/image'\ntest_mask_dir = '/kaggle/input/lung-mask-image-dataset/ChestXray/test/mask'\n\n\nimage_size = (128, 128)  # Rozmiar do którego będą skalowane obrazy i maski\nbatch_size = 16  # Rozmiar partii danych","metadata":{"execution":{"iopub.status.busy":"2024-03-05T17:38:34.924314Z","iopub.execute_input":"2024-03-05T17:38:34.925207Z","iopub.status.idle":"2024-03-05T17:38:34.930484Z","shell.execute_reply.started":"2024-03-05T17:38:34.925173Z","shell.execute_reply":"2024-03-05T17:38:34.929613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport os \n\ndef process_path(image_path, mask_path):\n    # Wczytanie i przetworzenie obrazu\n    image = tf.io.read_file(image_path)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.resize(image, image_size)\n    image = image / 255.0  # Normalizacja do zakresu [0, 1]\n\n    # Wczytanie i przetworzenie maski\n    mask = tf.io.read_file(mask_path)\n    mask = tf.image.decode_png(mask, channels=1)\n    mask = tf.image.resize(mask, image_size)\n    mask = mask / 255.0  # Normalizacja do zakresu [0, 1]\n\n    return image, mask\n\n# Utworzenie listy ścieżek do obrazów i masek\nimage_paths = [os.path.join(train_image_dir, fname) for fname in os.listdir(train_image_dir)]\nmask_paths = [os.path.join(train_mask_dir, fname) for fname in os.listdir(train_mask_dir)]\n\n# Utworzenie tf.data.Dataset\ndataset = tf.data.Dataset.from_tensor_slices((image_paths, mask_paths))\ndataset = dataset.map(process_path, num_parallel_calls=tf.data.experimental.AUTOTUNE)\ndataset = dataset.batch(batch_size)\ndataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-05T17:38:36.808089Z","iopub.execute_input":"2024-03-05T17:38:36.808479Z","iopub.status.idle":"2024-03-05T17:38:37.506261Z","shell.execute_reply.started":"2024-03-05T17:38:36.808450Z","shell.execute_reply":"2024-03-05T17:38:37.505312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-03-05T17:41:07.835337Z","iopub.execute_input":"2024-03-05T17:41:07.836145Z","iopub.status.idle":"2024-03-05T17:41:07.841959Z","shell.execute_reply.started":"2024-03-05T17:41:07.836116Z","shell.execute_reply":"2024-03-05T17:41:07.840996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\n\ndef conv_block(input_tensor, num_filters):\n   encoder = layers.Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n   encoder = layers.BatchNormalization()(encoder)\n   encoder = layers.Activation('relu')(encoder)\n   encoder = layers.Conv2D(num_filters, (3, 3), padding='same')(encoder)\n   encoder = layers.BatchNormalization()(encoder)\n   encoder = layers.Activation('relu')(encoder)\n   return encoder\n\ndef encoder_block(input_tensor, num_filters):\n   encoder = conv_block(input_tensor, num_filters)\n   encoder_pool = layers.MaxPooling2D((2, 2), strides=(2, 2))(encoder)\n   return encoder_pool, encoder\n\ndef decoder_block(input_tensor, concat_tensor, num_filters):\n   decoder = layers.Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(input_tensor)\n   decoder = layers.concatenate([concat_tensor, decoder], axis=-1)\n   decoder = layers.BatchNormalization()(decoder)\n   decoder = layers.Activation('relu')(decoder)\n   decoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n   decoder = layers.BatchNormalization()(decoder)\n   decoder = layers.Activation('relu')(decoder)\n   decoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n   decoder = layers.BatchNormalization()(decoder)\n   decoder = layers.Activation('relu')(decoder)\n   return decoder\n\n\ndef build_unet(input_shape):\n   inputs = layers.Input(shape=input_shape)\n\n   # Encoder\n   encoder0_pool, encoder0 = encoder_block(inputs, 32)\n   encoder1_pool, encoder1 = encoder_block(encoder0_pool, 64)\n   encoder2_pool, encoder2 = encoder_block(encoder1_pool, 128)\n   encoder3_pool, encoder3 = encoder_block(encoder2_pool, 256)\n\n   # Center\n   center = conv_block(encoder3_pool, 512)\n\n   # Decoder\n   decoder3 = decoder_block(center, encoder3, 256)\n   decoder2 = decoder_block(decoder3, encoder2, 128)\n   decoder1 = decoder_block(decoder2, encoder1, 64)\n   decoder0 = decoder_block(decoder1, encoder0, 32)\n\n   # Output\n   outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(decoder0)\n\n   model = models.Model(inputs=[inputs], outputs=[outputs])\n   return model\n","metadata":{"execution":{"iopub.status.busy":"2024-03-05T17:40:04.810331Z","iopub.execute_input":"2024-03-05T17:40:04.810693Z","iopub.status.idle":"2024-03-05T17:40:04.824708Z","shell.execute_reply.started":"2024-03-05T17:40:04.810666Z","shell.execute_reply":"2024-03-05T17:40:04.823631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=build_unet(input_shape=(128, 128, 3))\nmodel.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy', metrics=['accuracy'])\nmodel.fit(dataset, epochs=10, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T17:45:40.651328Z","iopub.execute_input":"2024-03-05T17:45:40.652066Z","iopub.status.idle":"2024-03-05T18:01:59.874870Z","shell.execute_reply.started":"2024-03-05T17:45:40.652033Z","shell.execute_reply":"2024-03-05T18:01:59.873855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_paths = [os.path.join(test_image_dir, fname) for fname in os.listdir(test_image_dir)]\nmask_paths = [os.path.join(test_mask_dir, fname) for fname in os.listdir(test_mask_dir)]\n\n# Utworzenie tf.data.Dataset\ndataset_test = tf.data.Dataset.from_tensor_slices((image_paths, mask_paths))\ndataset_test = dataset_test.map(process_path, num_parallel_calls=tf.data.experimental.AUTOTUNE)\ndataset_test = dataset_test.batch(batch_size)\ndataset_test = dataset_test.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-05T18:04:55.797384Z","iopub.execute_input":"2024-03-05T18:04:55.798089Z","iopub.status.idle":"2024-03-05T18:04:55.857103Z","shell.execute_reply.started":"2024-03-05T18:04:55.798059Z","shell.execute_reply":"2024-03-05T18:04:55.856360Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decoded_images = model.predict(dataset_test)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T18:04:57.632272Z","iopub.execute_input":"2024-03-05T18:04:57.632647Z","iopub.status.idle":"2024-03-05T18:05:11.982268Z","shell.execute_reply.started":"2024-03-05T18:04:57.632617Z","shell.execute_reply":"2024-03-05T18:05:11.981175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"execution":{"iopub.status.busy":"2024-03-05T18:08:46.282473Z","iopub.execute_input":"2024-03-05T18:08:46.283112Z","iopub.status.idle":"2024-03-05T18:08:46.296917Z","shell.execute_reply.started":"2024-03-05T18:08:46.283080Z","shell.execute_reply":"2024-03-05T18:08:46.296127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nn = 20 # Wybierz liczbę przykładów do wyświetlenia\nfor i in range(n):\n    plt.imshow(decoded_images[i],cmap='binary')\n    plt.title(\"Rekonstrukcja\")\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-05T18:21:02.173400Z","iopub.execute_input":"2024-03-05T18:21:02.174063Z","iopub.status.idle":"2024-03-05T18:21:07.273158Z","shell.execute_reply.started":"2024-03-05T18:21:02.174033Z","shell.execute_reply":"2024-03-05T18:21:07.272256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decoded_images[0]-decoded_images[1]","metadata":{"execution":{"iopub.status.busy":"2024-03-05T18:11:04.832593Z","iopub.execute_input":"2024-03-05T18:11:04.833205Z","iopub.status.idle":"2024-03-05T18:11:04.840290Z","shell.execute_reply.started":"2024-03-05T18:11:04.833174Z","shell.execute_reply":"2024-03-05T18:11:04.839322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}