{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1929264,"sourceType":"datasetVersion","datasetId":1150837},{"sourceId":6268840,"sourceType":"datasetVersion","datasetId":3603518}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Opis Zadania\n\nTwoim celem jest przetestowanie i przystosowanie do dobrego treningu modelu seq-to-seq, który będzie mógł rozwiązać problem tłumaczenia automatycznego. Model ma nauczyć się na podstawie zbioru danych tekstowych ```pol_neg.csv``` i nauczyć się przewidywać kolejne słowa na podstawie częściowo wprowadzonego zdania.\n\n**Instrukcje**\n\n1. Zaimportuj odpowiednie moduły z biblioteki Keras i pobierz dane do treningu.\n\n2. Uruchom architekturę modelu seq-to-seq z wykorzystaniem warstw LSTM. Model  ma enkoder i dekoder, a także warstwę gęstą na końcu dekodera.\n\n3. Skonfiguruj model i wybierz optymalizator learning rate oraz funkcję straty. Dodaj liczbę epoch i odpowiedni batch size.\n\n4. Trenuj model na  danych treningowych.\n\n6. Oceń wyniki treningu i przetestuj działanie modelu.\n\nZapisz wytrenowany model do pliku, aby mógł być używany w przyszłości.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv('/kaggle/input/ankiweb-polish-english/pol_eng.csv')\ndf.head()\n\ninput_texts = df['angielski']\ntarget_texts = df['polski']\ntarget_texts=target_texts.str.replace('\\n','')\ntarget_texts=target_texts.str.replace('\\t','')\ndf['polski']=target_texts\n# Utwórz zbiór unikalnych znaków dla polskiego i angielskiego\ninput_characters = set().union(*df['angielski'].apply(set))\ntarget_characters = set().union(*df['polski'].apply(set))\n\n\nnum_encoder_tokens = len(sorted(list(input_characters)))\nnum_decoder_tokens = len( sorted(list(target_characters)))\nmax_encoder_seq_length = max([len(txt) for txt in input_texts])\nmax_decoder_seq_length = max([len(txt) for txt in target_texts])\n\nprint('Number of samples:', len(input_texts))\nprint('Number of unique input (PL) tokens:', num_encoder_tokens)\nprint('Number of unique output (EN) tokens:', num_decoder_tokens)\nprint('Max sequence length for inputs (PL):', max_encoder_seq_length)\nprint('Max sequence length for outputs (EN):', max_decoder_seq_length)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-13T18:21:45.213093Z","iopub.execute_input":"2024-02-13T18:21:45.213543Z","iopub.status.idle":"2024-02-13T18:21:45.317609Z","shell.execute_reply.started":"2024-02-13T18:21:45.213502Z","shell.execute_reply":"2024-02-13T18:21:45.316452Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Number of samples: 10000\nNumber of unique input (PL) tokens: 71\nNumber of unique output (EN) tokens: 86\nMax sequence length for inputs (PL): 19\nMax sequence length for outputs (EN): 56\n","output_type":"stream"}]},{"cell_type":"code","source":"target_texts=target_texts.str.replace('\\n','')\ntarget_texts=target_texts.str.replace('\\t','')\n","metadata":{"execution":{"iopub.status.busy":"2024-02-13T18:19:02.906736Z","iopub.execute_input":"2024-02-13T18:19:02.907867Z","iopub.status.idle":"2024-02-13T18:19:02.929852Z","shell.execute_reply.started":"2024-02-13T18:19:02.907822Z","shell.execute_reply":"2024-02-13T18:19:02.928663Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"df['polski']","metadata":{"execution":{"iopub.status.busy":"2024-02-13T18:21:16.320274Z","iopub.execute_input":"2024-02-13T18:21:16.320696Z","iopub.status.idle":"2024-02-13T18:21:16.329704Z","shell.execute_reply.started":"2024-02-13T18:21:16.320663Z","shell.execute_reply":"2024-02-13T18:21:16.328621Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"0                        Idź.\n1                      Cześć.\n2                    Uciekaj!\n3                    Biegnij.\n4                    Uciekaj.\n                ...          \n9995     Tom zmienił pociągi.\n9996      Tom przekonał Mary.\n9997    Tom gotuje dla Marii.\n9998    Tom mógłby wam pomóc.\n9999     Tom mógłby ci pomóc.\nName: polski, Length: 10000, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"!wget https://github.com/marcin119a/PUMP2/raw/main/decoder_input_data.npz & wget https://github.com/marcin119a/PUMP2/raw/main/decoder_target_data.npz & wget https://github.com/marcin119a/PUMP2/raw/main/encoder_input_data.npz","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:17:35.798903Z","iopub.execute_input":"2024-02-13T17:17:35.799640Z","iopub.status.idle":"2024-02-13T17:17:37.417825Z","shell.execute_reply.started":"2024-02-13T17:17:35.799601Z","shell.execute_reply":"2024-02-13T17:17:37.416376Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"--2024-02-13 17:17:36--  https://github.com/marcin119a/PUMP2/raw/main/decoder_target_data.npz\n--2024-02-13 17:17:36--  https://github.com/marcin119a/PUMP2/raw/main/decoder_input_data.npz\n--2024-02-13 17:17:36--  https://github.com/marcin119a/PUMP2/raw/main/encoder_input_data.npz\nResolving github.com (github.com)... Resolving github.com (github.com)... Resolving github.com (github.com)... 140.82.112.4\nConnecting to github.com (github.com)|140.82.112.4|:443... 140.82.112.4\nConnecting to github.com (github.com)|140.82.112.4|:443... 140.82.112.3\nConnecting to github.com (github.com)|140.82.112.3|:443... connected.\nconnected.\nconnected.\nHTTP request sent, awaiting response... HTTP request sent, awaiting response... HTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/marcin119a/PUMP2/main/decoder_target_data.npz [following]\n--2024-02-13 17:17:37--  https://raw.githubusercontent.com/marcin119a/PUMP2/main/decoder_target_data.npz\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... 302 Found\nLocation: https://raw.githubusercontent.com/marcin119a/PUMP2/main/encoder_input_data.npz [following]\n--2024-02-13 17:17:37--  https://raw.githubusercontent.com/marcin119a/PUMP2/main/encoder_input_data.npz\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... connected.\n185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... 302 Found\nLocation: https://raw.githubusercontent.com/marcin119a/PUMP2/main/decoder_input_data.npz [following]\n--2024-02-13 17:17:37--  https://raw.githubusercontent.com/marcin119a/PUMP2/main/decoder_input_data.npz\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... connected.\nHTTP request sent, awaiting response... HTTP request sent, awaiting response... 200 OK\nLength: 1341001 (1.3M) [application/octet-stream]\nSaving to: ‘decoder_target_data.npz’\n\ndecoder_target_data   0%[                    ]       0  --.-KB/s               200 OK\nLength: 1339417 (1.3M) [application/octet-stream]\nSaving to: ‘decoder_input_data.npz’\n\ndecoder_target_data 100%[===================>]   1.28M  --.-KB/s    in 0.07s   \n\n2024-02-13 17:17:37 (18.6 MB/s) - ‘decoder_target_data.npz’ saved [1341001/1341001]\n\ndecoder_input_data. 100%[===================>]   1.28M  --.-KB/s    in 0.07s   \n\n2024-02-13 17:17:37 (19.6 MB/s) - ‘decoder_input_data.npz’ saved [1339417/1339417]\n\n200 OK\nLength: 463273 (452K) [application/octet-stream]\nSaving to: ‘encoder_input_data.npz’\n\nencoder_input_data. 100%[===================>] 452.42K  --.-KB/s    in 0.05s   \n\n2024-02-13 17:17:37 (9.34 MB/s) - ‘encoder_input_data.npz’ saved [463273/463273]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\n# Wczytujemy arrayy ze skompresowanego pliku .npz\ndecoder_input_data = np.load('decoder_input_data.npz')['decoder_input_data']\ndecoder_target_data = np.load('decoder_target_data.npz')['my_array']\nencoder_input_data = np.load('encoder_input_data.npz')['encoder_input_data']","metadata":{"execution":{"iopub.status.busy":"2024-02-13T18:21:27.373671Z","iopub.execute_input":"2024-02-13T18:21:27.374846Z","iopub.status.idle":"2024-02-13T18:21:28.202893Z","shell.execute_reply.started":"2024-02-13T18:21:27.374779Z","shell.execute_reply":"2024-02-13T18:21:28.201958Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### TODO trening modelu do wysokiego accuracy","metadata":{}},{"cell_type":"code","source":"latent_dim = 512\nbatch_size = 1024\nepochs = 60\n\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.models import Model\nfrom keras.layers import Input, LSTM, Dense\n\n# Zdefiniuj sekwencję wejściową i przetwórz ją.\nencoder_inputs = Input(shape=(None, num_encoder_tokens))\nencoder = LSTM(latent_dim, return_state=True)\nencoder_outputs, state_h, state_c = encoder(encoder_inputs)\n# Odrzucamy `encoder_outputs` i zachowujemy tylko stany.\nencoder_states = [state_h, state_c]\n\n# Skonfiguruj dekoder, używając `encoder_states` jako stanu początkowego.\ndecoder_inputs = Input(shape=(None, num_decoder_tokens))\n# Ustawiamy dekoder tak, aby zwracał pełne sekwencje wyjściowe,\n# oraz zwracał również stany wewnętrzne. Nie używamy stanów w modelu treningowym,\n# ale wykorzystamy je w predykcji.\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\ndecoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n                                     initial_state=encoder_states)\ndecoder_dense = Dense(num_decoder_tokens, activation='softmax')\ndecoder_outputs = decoder_dense(decoder_outputs)\n\n# Zdefiniuj model, który zamieni `encoder_input_data` i `decoder_input_data` w `decoder_target_data`.\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n\n# Uruchom trening\nmodel.compile(optimizer=Adam(learning_rate=0.004), loss='categorical_crossentropy',\n              metrics=['accuracy'])\nmodel.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n          batch_size=batch_size,\n          epochs=epochs,\n          validation_split=0.2)\n# Zapisz model\nmodel.save('s2s.h5')\n","metadata":{"execution":{"iopub.status.busy":"2024-02-13T18:21:29.497902Z","iopub.execute_input":"2024-02-13T18:21:29.498302Z","iopub.status.idle":"2024-02-13T18:21:31.323243Z","shell.execute_reply.started":"2024-02-13T18:21:29.498274Z","shell.execute_reply":"2024-02-13T18:21:31.321669Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Epoch 1/60\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Uruchom trening\u001b[39;00m\n\u001b[1;32m     31\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.004\u001b[39m), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     32\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mencoder_input_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_input_data\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_target_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Zapisz model\u001b[39;00m\n\u001b[1;32m     38\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms2s.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/tmp/__autograph_generated_filehk0kzojy.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 1 of layer \"model\" is incompatible with the layer: expected shape=(None, None, 86), found shape=(None, 58, 88)\n"],"ename":"ValueError","evalue":"in user code:\n\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 1 of layer \"model\" is incompatible with the layer: expected shape=(None, None, 86), found shape=(None, 58, 88)\n","output_type":"error"}]},{"cell_type":"code","source":"input_token_index = dict(\n    [(char, i) for i, char in enumerate(input_characters)])\ntarget_token_index = dict(\n    [(char, i) for i, char in enumerate(target_characters)])\n\n# Kolejny krok: tryb wnioskowania (sampling).\n# Oto kroki do wykonania:\n# 1) Zakoduj dane wejściowe i pobierz początkowy stan dekodera.\n# 2) Uruchom jeden krok dekodera z tym początkowym stanem\n#    i tokenem \"początek sekwencji\" jako cel.\n#    Wynikiem będzie następny token celu.\n# 3) Powtórz z bieżącym tokenem celu i bieżącymi stanami.\n\n\n# Define sampling models\nencoder_model = Model(encoder_inputs, encoder_states)\n\n\ndecoder_states_inputs = [Input(shape=(latent_dim,)), Input(shape=(latent_dim,))]\ndecoder_outputs, state_h, state_c = decoder_lstm(\n    decoder_inputs, initial_state=decoder_states_inputs)\ndecoder_states = [state_h, state_c]\ndecoder_outputs = decoder_dense(decoder_outputs)\ndecoder_model = Model(\n    [decoder_inputs] + decoder_states_inputs,\n    [decoder_outputs] + decoder_states)\n\n# indeks tokena wyszukiwania wstecznego do dekodowania sekwencji z powrotem do\n# coś czytelnego.\nreverse_input_char_index = dict(\n    (i, char) for char, i in input_token_index.items())\nreverse_target_char_index = dict(\n    (i, char) for char, i in target_token_index.items())\n\n\ndef decode_sequence(input_seq):\n    # Zakoduj dane wejściowe w postaci wektorów stanu.\n    states_value = encoder_model.predict(input_seq)\n\n    # Wygeneruj pustą sekwencję celu o długości 1.\n    target_seq = np.zeros((1, 1, num_decoder_tokens))\n    # Wypełnij pierwszy znak sekwencji celu znakiem początkowym.\n    target_seq[0, 0, target_token_index['\\t']] = 1.\n\n    # Pętla losowania dla jednej sekwencji (dla uproszczenia, zakładamy rozmiar batcha równy 1).\n    stop_condition = False\n    decoded_sentence = ''\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict([target_seq] + states_value, verbose=0)\n\n        # Wylosuj token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_char = reverse_target_char_index[sampled_token_index]\n        decoded_sentence += sampled_char\n\n        # Warunek zakończenia pętli: osiągnięcie maksymalnej długości sekwencji\n        # lub znalezienie znaku zakończenia.\n        if (sampled_char == '\\n' or\n           len(decoded_sentence) > max_decoder_seq_length):\n            stop_condition = True\n\n        # Aktualizuj sekwencję celu (o długości 1).\n        target_seq = np.zeros((1, 1, num_decoder_tokens))\n        target_seq[0, 0, sampled_token_index] = 1.\n\n        # Aktualizuj stany\n        states_value = [h, c]\n\n    return decoded_sentence\n\n\nfor seq_index in range(10):\n    # Wybierz jedną sekwencję (część zestawu treningowego)\n    # do wypróbowania dekodowania.\n    input_seq = encoder_input_data[seq_index: seq_index + 1]\n    decoded_sentence = decode_sequence(input_seq)\n    print('-')\n    print('Input sentence:', input_texts[seq_index])\n    print('Decoded sentence:', decoded_sentence)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:22:56.244828Z","iopub.execute_input":"2024-02-13T17:22:56.245227Z","iopub.status.idle":"2024-02-13T17:23:09.689547Z","shell.execute_reply.started":"2024-02-13T17:22:56.245198Z","shell.execute_reply":"2024-02-13T17:23:09.687877Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 0s 489ms/step\n-\nInput sentence: Go.\nDecoded sentence: wśvFśfFŹfFŹdńn:Ź56FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF\n1/1 [==============================] - 0s 25ms/step\n-\nInput sentence: Hi.\nDecoded sentence: s:ń:FŹf56FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF\n1/1 [==============================] - 0s 23ms/step\n-\nInput sentence: Run!\nDecoded sentence: BHĆ\n\n1/1 [==============================] - 0s 23ms/step\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 76\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seq_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# Wybierz jedną sekwencję (część zestawu treningowego)\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# do wypróbowania dekodowania.\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     input_seq \u001b[38;5;241m=\u001b[39m encoder_input_data[seq_index: seq_index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 76\u001b[0m     decoded_sentence \u001b[38;5;241m=\u001b[39m \u001b[43mdecode_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput sentence:\u001b[39m\u001b[38;5;124m'\u001b[39m, input_texts[seq_index])\n","Cell \u001b[0;32mIn[8], line 49\u001b[0m, in \u001b[0;36mdecode_sequence\u001b[0;34m(input_seq)\u001b[0m\n\u001b[1;32m     47\u001b[0m decoded_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stop_condition:\n\u001b[0;32m---> 49\u001b[0m     output_tokens, h, c \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_seq\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstates_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# Wylosuj token\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     sampled_token_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(output_tokens[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/training.py:2378\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2376\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_predict_begin()\n\u001b[1;32m   2377\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2378\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, iterator \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39menumerate_epochs():  \u001b[38;5;66;03m# Single epoch.\u001b[39;00m\n\u001b[1;32m   2379\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m   2380\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/data_adapter.py:1305\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1303\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[0;32m-> 1305\u001b[0m     data_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1306\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epochs):\n\u001b[1;32m   1307\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:505\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[1;32m    504\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[0;32m--> 505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    507\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:713\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    709\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    710\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    711\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    712\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 713\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_call_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:752\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    749\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fulltype\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[1;32m    750\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types)\n\u001b[1;32m    751\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[0;32m--> 752\u001b[0m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3408\u001b[0m, in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m   3407\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3408\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3409\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMakeIterator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   3411\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"### Zadanie: Klasyfikacja języka na podstawie pojedynczych liter\n\nTwoim zadaniem jest stworzenie modelu sieci neuronowej, który będzie w stanie klasyfikować teksty w różnych językach na podstawie pojedynczych liter. W zbiorze danych znajdują się teksty w języku angielskim i polskim, a Twoim celem jest nauczenie modelu rozpoznawania, do którego języka należy dany tekst.\n\nKroki, które musisz podjąć:\n\n1. **Przygotowanie danych**: Dane tekstowe zostały już wczytane z pliku CSV. W zbiorze danych znajdują się teksty w języku angielskim i polskim. Każdy tekst został podzielony na pojedyncze litery, a litery zostały przekształcone na sekwencje liczb. Twoim zadaniem jest podzielenie danych na zbiór uczący (training) i zbiór testowy (test).\n\n2. **Model sieci neuronowej**: Zdefiniuj model sieci rekurencyjnej. Jest to model oparty na warstwach Embedding, LSTM i GlobalMaxPooling1D, z warstwami Dense na końcu. Model ma za zadanie klasyfikować teksty na podstawie pojedynczych liter.\n\n3. **Kompilacja modelu**: Model musi zostać skompilowany z odpowiednim optymalizatorem ('adam') i funkcją straty ('binary_crossentropy'). Ponieważ mamy problem binarnej klasyfikacji języka, używamy funkcji 'binary_crossentropy'.\n\n4. **Trenowanie modelu**: Trenuj model na danych treningowych. W kodzie już jest zaimplementowane trenowanie na jednej epoce (epochs=1), ale możesz dostosować to do swoich potrzeb. Warto eksperymentować z różnymi liczbami epok i innymi parametrami trenowania.\n\n5. **Ewaluacja modelu**: Po zakończeniu trenowania, ocen model na danych testowych. Sprawdź, jak dobrze model klasyfikuje teksty na podstawie pojedynczych liter, i oblicz dokładność klasyfikacji (accuracy).\n\nTwoim celem jest stworzenie modelu, który będzie w stanie skutecznie rozpoznawać język na podstawie pojedynczych liter. Możesz eksperymentować z architekturą modelu, parametrami trenowania i innymi aspektami, aby osiągnąć jak najlepsze wyniki.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd \n\ndf = pd.read_csv('/kaggle/input/ankiweb-polish-english/pol_eng.csv')","metadata":{"execution":{"iopub.status.busy":"2024-02-13T18:22:41.345457Z","iopub.execute_input":"2024-02-13T18:22:41.346494Z","iopub.status.idle":"2024-02-13T18:22:41.368904Z","shell.execute_reply.started":"2024-02-13T18:22:41.346456Z","shell.execute_reply":"2024-02-13T18:22:41.368033Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"target_texts = df['polski']\ntarget_texts=target_texts.str.replace('\\n','')\ntarget_texts=target_texts.str.replace('\\t','')\ndf['polski']=target_texts","metadata":{"execution":{"iopub.status.busy":"2024-02-13T18:22:43.323827Z","iopub.execute_input":"2024-02-13T18:22:43.324751Z","iopub.status.idle":"2024-02-13T18:22:43.348275Z","shell.execute_reply.started":"2024-02-13T18:22:43.324717Z","shell.execute_reply":"2024-02-13T18:22:43.347272Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-02-13T18:22:46.556481Z","iopub.execute_input":"2024-02-13T18:22:46.557025Z","iopub.status.idle":"2024-02-13T18:22:46.572793Z","shell.execute_reply.started":"2024-02-13T18:22:46.556975Z","shell.execute_reply":"2024-02-13T18:22:46.571455Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"      Unnamed: 0            angielski                 polski\n0              0                  Go.                   Idź.\n1              1                  Hi.                 Cześć.\n2              2                 Run!               Uciekaj!\n3              3                 Run.               Biegnij.\n4              4                 Run.               Uciekaj.\n...          ...                  ...                    ...\n9995        9995  Tom changed trains.   Tom zmienił pociągi.\n9996        9996  Tom convinced Mary.    Tom przekonał Mary.\n9997        9997  Tom cooks for Mary.  Tom gotuje dla Marii.\n9998        9998  Tom could help you.  Tom mógłby wam pomóc.\n9999        9999  Tom could help you.   Tom mógłby ci pomóc.\n\n[10000 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>angielski</th>\n      <th>polski</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Go.</td>\n      <td>Idź.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Hi.</td>\n      <td>Cześć.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Run!</td>\n      <td>Uciekaj!</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Run.</td>\n      <td>Biegnij.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Run.</td>\n      <td>Uciekaj.</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9995</th>\n      <td>9995</td>\n      <td>Tom changed trains.</td>\n      <td>Tom zmienił pociągi.</td>\n    </tr>\n    <tr>\n      <th>9996</th>\n      <td>9996</td>\n      <td>Tom convinced Mary.</td>\n      <td>Tom przekonał Mary.</td>\n    </tr>\n    <tr>\n      <th>9997</th>\n      <td>9997</td>\n      <td>Tom cooks for Mary.</td>\n      <td>Tom gotuje dla Marii.</td>\n    </tr>\n    <tr>\n      <th>9998</th>\n      <td>9998</td>\n      <td>Tom could help you.</td>\n      <td>Tom mógłby wam pomóc.</td>\n    </tr>\n    <tr>\n      <th>9999</th>\n      <td>9999</td>\n      <td>Tom could help you.</td>\n      <td>Tom mógłby ci pomóc.</td>\n    </tr>\n  </tbody>\n</table>\n<p>10000 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, GlobalMaxPooling1D\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import pad_sequences\n\n# Przygotowanie danych próbkowych (zbiór tekstów w różnych językach)\ntexts = df['angielski'].to_list() + df['polski'].to_list()\n\n# Pojedyncze litery\nletter_texts = [' '.join(list(text)) for text in texts]\n\nlabels = np.repeat(np.arange(2), 10000)\n\n# Tokenizacja i konwersja tekstów na sekwencje liczb\ntokenizer = Tokenizer(char_level=True)  # Ustawienie char_level na True\ntokenizer.fit_on_texts(letter_texts)\nsequences = tokenizer.texts_to_sequences(letter_texts)\n\n# Wypełnienie sekwencji do tej samej długości\nmax_len = max([len(seq) for seq in sequences])\nsequences = pad_sequences(sequences, maxlen=max_len)\n\n# Teraz sequences zawiera ztokenizowane litery.\n","metadata":{"execution":{"iopub.status.busy":"2024-02-13T18:22:48.985088Z","iopub.execute_input":"2024-02-13T18:22:48.986136Z","iopub.status.idle":"2024-02-13T18:22:49.650036Z","shell.execute_reply.started":"2024-02-13T18:22:48.986095Z","shell.execute_reply":"2024-02-13T18:22:49.649066Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(sequences, labels, test_size=0.2, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-13T18:22:56.391059Z","iopub.execute_input":"2024-02-13T18:22:56.391494Z","iopub.status.idle":"2024-02-13T18:22:56.403478Z","shell.execute_reply.started":"2024-02-13T18:22:56.391457Z","shell.execute_reply":"2024-02-13T18:22:56.402251Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"labels","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:31:29.905307Z","iopub.execute_input":"2024-02-13T17:31:29.906363Z","iopub.status.idle":"2024-02-13T17:31:29.913257Z","shell.execute_reply.started":"2024-02-13T17:31:29.906324Z","shell.execute_reply":"2024-02-13T17:31:29.912067Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"array([0, 0, 0, ..., 1, 1, 1])"},"metadata":{}}]},{"cell_type":"code","source":"#todo model\nmodel=Sequential()\nmodel.add(Embedding(input_dim=115,output_dim=115))\nmodel.add(LSTM(115))\n#model.add(GlobalMaxPooling1D())\n#model.add(Dense(64, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.05), metrics=['accuracy'])\nmodel.fit(X_train,y_train, epochs=10, batch_size=1024, validation_split=0.2)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-13T18:25:51.362627Z","iopub.execute_input":"2024-02-13T18:25:51.363062Z","iopub.status.idle":"2024-02-13T18:26:02.264848Z","shell.execute_reply.started":"2024-02-13T18:25:51.363029Z","shell.execute_reply":"2024-02-13T18:26:02.263704Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Epoch 1/10\n13/13 [==============================] - 4s 170ms/step - loss: 0.6523 - accuracy: 0.6311 - val_loss: 0.3805 - val_accuracy: 0.8409\nEpoch 2/10\n13/13 [==============================] - 1s 89ms/step - loss: 0.2178 - accuracy: 0.9120 - val_loss: 0.0938 - val_accuracy: 0.9706\nEpoch 3/10\n13/13 [==============================] - 1s 55ms/step - loss: 0.0699 - accuracy: 0.9793 - val_loss: 0.0402 - val_accuracy: 0.9853\nEpoch 4/10\n13/13 [==============================] - 1s 43ms/step - loss: 0.0367 - accuracy: 0.9873 - val_loss: 0.0282 - val_accuracy: 0.9916\nEpoch 5/10\n13/13 [==============================] - 1s 59ms/step - loss: 0.0280 - accuracy: 0.9904 - val_loss: 0.0222 - val_accuracy: 0.9928\nEpoch 6/10\n13/13 [==============================] - 1s 43ms/step - loss: 0.0221 - accuracy: 0.9918 - val_loss: 0.0226 - val_accuracy: 0.9928\nEpoch 7/10\n13/13 [==============================] - 1s 55ms/step - loss: 0.0121 - accuracy: 0.9959 - val_loss: 0.0143 - val_accuracy: 0.9959\nEpoch 8/10\n13/13 [==============================] - 1s 43ms/step - loss: 0.0076 - accuracy: 0.9978 - val_loss: 0.0167 - val_accuracy: 0.9937\nEpoch 9/10\n13/13 [==============================] - 1s 43ms/step - loss: 0.0105 - accuracy: 0.9963 - val_loss: 0.0153 - val_accuracy: 0.9947\nEpoch 10/10\n13/13 [==============================] - 1s 44ms/step - loss: 0.0122 - accuracy: 0.9955 - val_loss: 0.0166 - val_accuracy: 0.9953\n","output_type":"stream"},{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7a8f44b89600>"},"metadata":{}}]},{"cell_type":"code","source":"model.evaluate(X_test, y_test, verbose=0)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T18:26:16.221174Z","iopub.execute_input":"2024-02-13T18:26:16.221601Z","iopub.status.idle":"2024-02-13T18:26:16.914482Z","shell.execute_reply.started":"2024-02-13T18:26:16.221566Z","shell.execute_reply":"2024-02-13T18:26:16.913376Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"[0.021389856934547424, 0.9925000071525574]"},"metadata":{}}]},{"cell_type":"markdown","source":"\n---\n\n### Zadanie: Detekcja języka tekstu\n\nTwoim zadaniem jest stworzenie modelu, który będzie w stanie rozpoznawać język tekstu na podstawie dostępnych danych. W zbiorze danych znajdują się teksty w różnych językach, a Twoim celem jest nauczenie modelu identyfikować język każdego tekstu.\n\nKroki, które musisz podjąć:\n\n1. **Przygotowanie danych**: Dane tekstowe zostały już wczytane z pliku CSV. W zbiorze danych znajdują się teksty w różnych językach. Każdy tekst został podzielony na sekwencje słów, a słowa zostały przekształcone na sekwencje liczb. Twoim zadaniem jest podzielenie danych na zbiór uczący (training) i zbiór testowy (test).\n\n2. **Mapowanie języków**: Przygotuj mapowanie języków na unikalne identyfikatory, które będzie można użyć w modelu. Mapowanie zostało już zaimplementowane w kodzie.\n\n3. **Tokenizacja tekstu**: Przekształć tekst na sekwencje liczbowe za pomocą tokenizacji. Ustaw stałą długość sekwencji (np. przez dodawanie zer na końcu sekwencji).\n\n4. **Model sieci neuronowej**: Przygoutj model sieci neuronowej. Jest to model oparty na warstwach Embedding, LSTM i Dense. Model ma za zadanie klasyfikować teksty na podstawie języka.\n\n5. **Kompilacja modelu**: Model musi zostać skompilowany z odpowiednim optymalizatorem ('adam') i funkcją straty ('categorical_crossentropy'). Ponieważ mamy problem wieloklasowej klasyfikacji języka, używamy funkcji straty 'categorical_crossentropy'.\n\n6. **Trenowanie modelu**: Trenuj model na danych treningowych. Możesz dostosować liczbę epok, rozmiar wsadu (batch size) i inne parametry trenowania, aby uzyskać jak najlepsze wyniki.\n\n7. **Ewaluacja modelu**: Po zakończeniu trenowania, ocen model na danych testowych. Sprawdź, jak dobrze model identyfikuje język tekstów, i oblicz dokładność klasyfikacji (accuracy).\n\nTwoim celem jest stworzenie modelu, który będzie w stanie skutecznie rozpoznawać język tekstu na podstawie dostępnych danych. Możesz eksperymentować z architekturą modelu, parametrami trenowania i innymi aspektami, aby osiągnąć jak najlepsze wyniki.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd \n\ndf = pd.read_csv('/kaggle/input/language-detection/Language Detection.csv')","metadata":{"execution":{"iopub.status.busy":"2024-02-13T18:59:59.291940Z","iopub.execute_input":"2024-02-13T18:59:59.292419Z","iopub.status.idle":"2024-02-13T18:59:59.360503Z","shell.execute_reply.started":"2024-02-13T18:59:59.292385Z","shell.execute_reply":"2024-02-13T18:59:59.359577Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ntexts = df['Text'].values\nlanguages = df['Language'].values\n\n# Przygotuj mapowanie języków na unikalne identyfikatory\nlanguage_to_id = {lang: i for i, lang in enumerate(set(languages))}\nid_to_language = {i: lang for lang, i in language_to_id.items()}\nnum_languages = len(language_to_id)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-13T19:00:00.929707Z","iopub.execute_input":"2024-02-13T19:00:00.930442Z","iopub.status.idle":"2024-02-13T19:00:00.939682Z","shell.execute_reply.started":"2024-02-13T19:00:00.930407Z","shell.execute_reply":"2024-02-13T19:00:00.938692Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"language_to_id\n","metadata":{"execution":{"iopub.status.busy":"2024-02-13T19:05:39.480885Z","iopub.execute_input":"2024-02-13T19:05:39.481744Z","iopub.status.idle":"2024-02-13T19:05:39.488736Z","shell.execute_reply.started":"2024-02-13T19:05:39.481692Z","shell.execute_reply":"2024-02-13T19:05:39.487678Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"{'Malayalam': 0,\n 'Portugeese': 1,\n 'Italian': 2,\n 'French': 3,\n 'Dutch': 4,\n 'Turkish': 5,\n 'Sweedish': 6,\n 'Kannada': 7,\n 'Russian': 8,\n 'Danish': 9,\n 'Tamil': 10,\n 'Greek': 11,\n 'English': 12,\n 'German': 13,\n 'Hindi': 14,\n 'Arabic': 15,\n 'Spanish': 16}"},"metadata":{}}]},{"cell_type":"code","source":"#todo tokenizer i model + pad_sequences\n# Tokenizacja i konwersja tekstów na sekwencje liczb\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, GlobalMaxPooling1D\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\nfrom keras.utils import pad_sequences\nimport numpy as np\nfrom tensorflow.keras.optimizers import Adam\n\n# Tokenizacja tekstu\ntokenizer = Tokenizer(char_level=True)\ntokenizer.fit_on_texts(texts)\nsequences = tokenizer.texts_to_sequences(texts)\n\n# Ustawienie stałej długości sekwencji\nmax_len = max([len(seq) for seq in sequences])\nsequences = pad_sequences(sequences, maxlen=max_len)\n\n# Podział danych na zbiór treningowy i testowy\nX_train, X_test, y_train, y_test = train_test_split(sequences, languages, test_size=0.2, random_state=42)\n\n# Przygotowanie etykiet w postaci kategorialnej\ny_train = to_categorical([language_to_id[lang] for lang in y_train], num_classes=num_languages)\ny_test = to_categorical([language_to_id[lang] for lang in y_test], num_classes=num_languages)\n\n# Definicja modelu\nmodel = Sequential()\nmodel.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=64, input_length=max_len))\nmodel.add(LSTM(64, return_sequences=True))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(num_languages, activation='softmax'))\n\n# Kompilacja modelu\nmodel.compile(optimizer=Adam(learning_rate=0.05), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Trenowanie modelu\nmodel.fit(X_train, y_train, epochs=2, batch_size=128, validation_split=0.1)\n\n# Ewaluacja modelu\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(\"Test Accuracy:\", accuracy)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-13T19:00:02.998649Z","iopub.execute_input":"2024-02-13T19:00:02.999069Z","iopub.status.idle":"2024-02-13T19:01:54.035991Z","shell.execute_reply.started":"2024-02-13T19:00:02.999034Z","shell.execute_reply":"2024-02-13T19:01:54.034782Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Epoch 1/2\n59/59 [==============================] - 46s 727ms/step - loss: 0.9850 - accuracy: 0.6733 - val_loss: 0.5484 - val_accuracy: 0.8174\nEpoch 2/2\n59/59 [==============================] - 42s 707ms/step - loss: 0.3491 - accuracy: 0.8873 - val_loss: 0.3672 - val_accuracy: 0.8767\n65/65 [==============================] - 14s 209ms/step - loss: 0.3613 - accuracy: 0.8772\nTest Accuracy: 0.8771759867668152\n","output_type":"stream"}]},{"cell_type":"code","source":"#tokenizer.texts_to_sequences(['test'])\n\nmodel.predict(pad_sequences(tokenizer.texts_to_sequences(['humans']),maxlen=max_len))","metadata":{"execution":{"iopub.status.busy":"2024-02-13T19:05:47.904765Z","iopub.execute_input":"2024-02-13T19:05:47.905689Z","iopub.status.idle":"2024-02-13T19:05:48.187013Z","shell.execute_reply.started":"2024-02-13T19:05:47.905653Z","shell.execute_reply":"2024-02-13T19:05:48.185994Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 0s 223ms/step\n","output_type":"stream"},{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"array([[7.6992037e-05, 2.4113026e-02, 1.1043198e-02, 3.3211742e-02,\n        2.6673719e-01, 1.2736706e-01, 8.3265781e-02, 2.3529463e-05,\n        1.0524538e-03, 6.8296693e-02, 2.2484134e-03, 2.9869105e-03,\n        8.9769691e-02, 2.4984522e-01, 3.3598193e-03, 2.9332782e-03,\n        3.3669081e-02]], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}